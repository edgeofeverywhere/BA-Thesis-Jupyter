# BA-Thesis-Jupyter: Emergent Time-Invariant Inharmonic Timbral Feature Integration

## Description of Repository

This repository contains the workflow and process enumerated and successfully defended as an exemplar for the purposes of my BA Thesis. The first commit of the truncated, clean copy, `thesis_minimal-annotation.ipynb`, is what was submitted initially, alongside a messy scratch document I used to test things. However, I reorganized the code to make it a bit more readable by moving all the active parts of the codebase to the end, all the definitions and global objects to the front, and labeling each section, which I've pushed alongside this readme.

Bear in mind the functions are intended to be run in google colaboratory, and thereby has iPython Magic syntax in it that hooks to bash will not necessarily play nicely with a local run of the notebook. However, I might adapt and extend it again to be more accommodating to local use (moving the version submitted to an old branch) once I've acquired an NVIDIA card for my personal workstation. The amount of time crunch on the thesis that using Colab cost me owing to how slow the emulated I/O is was catastrophic (not to mention seemingly a even the smallest number of dropped packets over the sockets connecting me to the remote host would completely shut me out from my runtime), I've sworn it off since.

## Purpose / General Summary

The thesis endeavored to investigate the propagation of what I'm calling "mid-level" percepts as an empirical matter of informational integration. In the broader cognitive sciences, as a recent historical matter, hermeneutics that appear undergirded by an assumed ontology of information realism (such as IIT and Recurrent Processing theories), whereby iterated runs of functions understood as maximizing the density of subsymbolically embedded entropy per spatial dimension vis a vis loadings leads to the "emergence" of consciousness, there remain no concrete proposals or uniform frameworks for operationalizing these theories, to parsimoniously encapsulate them linguistically or mathematically, in a particular applied empirical case. To this end, the thesis sought to explore neural networks as a means by which new emergent accounts of "optimal" processes of dimensionality reduction, given a set structure, on tokenized units of information, might emerge. However, even if such mathematically precise methods are enumerated, it is my belief that such a mode of inquiry, without precise attention to the implicit epistemological scaffolding of the research itself, constitutive of the unique structure of transducers and the ultimate geometric form they may take, may not be necessarily generative or applicable to the human case, which is both more efficient in capacity per watt and structurally heterogeneous in terms of (inferred) structural composition. The often unspoken assumption is that, as embodied creatures who understand the world linguistically as an ordinal series of cause-and-effect type of relations, the most quickly fitting manner of course through which observed invariant relationships are made salient as perceptual categories within humans also possess temporal ordinality. In other words, we have no reason to assume that given an artificially delimited training set and number of epochs, the most naturally efficacious way to model fit perceptually stable forms from a continuous stream of information is to either simply randomize the order of tokens presented to the network per epoch or to present it to the network linearly.

From the perspective of the Bay Area literati ruling the roost in the West, this is of no concern. After all, the AI revolution is not about using, as has been since time immemorial, alchemical means to understand nature's creations. Rather, it is to construct the tower of Babel - it's about channeling productivity, parochialism, and prediction to ensure the maximal coordination of agents towards efficient production. This way, all is subsumed underneath a universal language - a locally enthalpic fever pitch that the lords of our nascent fiefdom promise will persist eternally. Ergo, the dream of the posthuman helmsman, a stochastically governed open society, can be realized. However, in this hunt for Absolute dominion, an epistemology that privileges the pride of navel-gazing excess and immanence is favored over a subdued and frugal naturalism. While such an approach is obviously possible with enough parameters and wattage, man will truly never render itself obsolete unless it eclipses, metric for metric, its own wetware in terms of performance. And before such resources as solid-state capable rare earth minerals are exhausted indefinitely, to put the Club of Rome's fear of God back into the equation, it's crucial that we get a handle on what latent efficiencies in structure, purely as a manner of temporal consideration, man actively hijacks in the subsumption of sensory information underneath extant conceptual unity, while such an opportunity remains available.

Luckily, we can operationalize and test this proposition directly, by comparing several types of training regimes constructed with various stochastically regular sampling processes across a delimited dataset and a limited number of epochs. Specifically, I used temporal windows selected through either a Rössler-Equation Dynamic System, a Maximal Entropy Random Walk (MERW), and an Ornstein-Uhlenbeck Process as experimental conditions, compared with the controls of both simple random sampling and linear presentation of said information. To this effect, the modality chosen for this investigation was audio, which, via the MP3 codec, already possess a well-understood and experimentally validated means of benchmarking complexity reduction in a high-dimensonal structure algorithmically in such a fashion whose finer niceties are cognitively impenetrable to the subject at high-enough bitrates. The .MP3 makes use of huffman-optimal bit allocation within the context of psychoacoustically modeled filterbanks for the purposes of maximizing so-called "perceptual entropy" in the resultant compressed file given the constraint of bitrate. Because said constraint is controllable in the encoding process, and a given codec itself is algorithmically determinate and produces checksum-valid artifacts given a constant bitrate, by precisely ablating the audio file to an extent where artifacts are readily noticable to human subjects (such as in the case of a low bitrate like 64kbps), we can train a variational autoencoder, when using the lossless representation as the error term, to construct a timbral feature space that is not, as traditional timbral spaces are, generally reducible to the rational limitations of the harmonic series expressed in the time-domain.

The potential upside to this method is we can now characterize the components of variation at a point where language itself regarding timbral description fails to capture the variation reliably. Hence - we can finally take a stab at the mid level problem - both in terms of content of the representation and the emergent rules governing the propagation of features, which can be analyzed in terms of the scale of the gradient changes to the evidence lower bound and the reparamaterized impact on sets of parameters within the encoder network. It is through a system-wide assessment and general characterization of these changes that general principles regarding time-optimal integration of elusive, mid-level feature sets can be derived. 

## Codebase Overview

The code, for simplicity, I'll explain in stages...

### Setup / Sample Set Acquisition

Prior to actually doing anything neural-network related, the code fetches a randomly sampled dataset from Kevin MacLeod's famous (or infamous) royalty free music website, incompetech.com, based off of the working URLs enumerated in the provided .txt file. The IDs of the sample songs for processing are recorded, and the directory structure for the plots and animations to be exported later, alongside locations for dumping of weights, are established in the working directory to keep things organized. Installation of dependencies are done in a colab specific way, and a necessary binding of a super to a class is applied to one of them - Brian2Hears. I should have used git to fork it (and maybe mapped a dependency graph to better isolate the implementation of the filterbank I needed), but as you can see I perl'd and regex'd my way into making the required changes, which was a hack-y workflow I was more accustomed to at the time. I didn't have time to remodel the chosen filterbank myself using SciPy (as Brian2Hears does) to return each individual filtered input for use (as it returns an object data structure that is specialized to that infrastructure) outside of Brian's suite of classes, which is why the override is provided. There is also the optional use of arrayfire - but since it uses a custom wheel outside of PyPi, and I found its implementation slower (largely because of iterated type conversion from numpy arrays to arrayfire ones requiring interfacing with system memory), I elected to not include it by default, though it can be re-enabled, and it may well be faster with a larger GPU memory pool that can handle the casting operation in GPU memory.

### Preprocessing

The model preprocessing largely occurs when running the configure_epochs() function, which handles all requisite calculations in a rolled and inline fashion, though I defined each helper function separately. It was important that the actual information pipelines were made interpretable pursuant to the overall purpose of the thesis, so strict attention had to be paid to the logic and construction of the object sets used in training. At the end, two sets of data will be produced for each file type - one for lossy versions of a file and one for lossless, which is controlled by the "with_lossy_backprop" **kwarg (which is set to true in this case). After the files are transcoded each to two mono signals (if use dual mono is true), we can proceed to process each channel according to a rough approximation of the cochlea's own attenuation to parts of the frequency spectrum.

I used the Approximated Gammatone Filterbank included in Brian2Hears to automatically generate 100 bandpass IIR Filters, following the filterbank design enumerated in Hohmann (2002), applied in equal sized bins along the time course of each sampled MacLeod track. Each bandpass filter, placed within the range of 20hz to 20khz, is centered according to its equivalent rectangular bandwidth relative to the natural logarithmic scale that defines the distribution of cilia along the course of the cochlea. The filter's q, is spaced and notched to target at an equivalent-rectangular bandwidth scale, calculated from the classic Glasberg and Moore (1983) parameter estimates. While the filterbank used in the creation of the actual .MP3 file in most codecs has a somewhat different architecture, using a Pseudo Quadrature Mirror Filter for reasons of aliasing reduction, as this model is meant to be made interpretable for the human case, biomimetic accuracy in terms of impulse shape, rather than simply training the model to perfectly reverse the original process, was favored. The temporal resolution for our model is constrained by the window length provided as a **kwarg to the function. These are then stacked as a numpy array for GPU-accelerated compute loads to be done, ensuring that each centroid is ordered properly along the length of one dimension as the portion of the frequency spectrum captured in each bin of a given window size increases.

Once this processing has completed, each set of windowed audio chunks is converted via discrete fourier transform to magnitude and phase values, using one of three backends - tensorflow, pytorch, or arrayfire, and placed into a nested dictionary structure. Here, the set of the 100 real and nonreal components of the dft-transformed signal for each of the filename's left and right channels, are labeled at a toplevel by filename + channel, with each filename + channel-pairing possessing dictionary entries corresponding to "real" and "imaginary" components of each bin. Both the real and imaginary keywords contain a list of bin_ordinal elements whose length is defined as the total number of samples per MacLeod track / window size elements.

```
# Per file, the output looks looks like ->
{
    'filename_channel1': {
        'real': {
            'bin_0': numpy.array([...]), # Shape: (window_size, 100)
            'bin_1': numpy.array([...]),
            ...
        },
        'imaginary': {
            'bin_0': numpy.array([...]),
            'bin_1': numpy.array([...]),
            ...
        }},
    'filename_channel2': {
        'real': {
            'bin_0': numpy.array([...]),
            'bin_1': numpy.array([...]),
             ...
        'imaginary': {
            'bin_0': numpy.array([...]),
            'bin_1': numpy.array([...]),
             ...
        }},
    }

```

### Dataset Shuffling / Stochastic Processing

From here, we can operationalize our stochastic methods of choice, using them to shuffle around the bins of the nested dictionary structure. For each filename_channel pairing, we need to ensure the same bin index, for both real and imaginary components of the signal, are shuffled to the same place in the data structure, to ensure that each batch we train the network on occurs at the same location per epoch. Hence, we bind matching bin_number keys using a union operator to easily ensure the same temporal reordering occurs to both phase-related and magnitude related information. These occur, similar to the preprocessing stage, as a single rolled inline run of the stochastic process in question of each for memory optimization reasons. Downstream functions inherit *args and **kwargs from the toplevel to enable more granular control of each parameter of the stochastic method selected. Below are precise details for each method's implementation within this context, and the rationale for any adaptations are made from their original articulations in the literature.

#### Ornstein-Uhlenbeck Implementaion

The Ornstein-Uhlenbeck process is implemented as a mean-reverting random walk, which provides a controlled way to explore the temporal space while maintaining some memory of previous states. The process is constrained by three key parameters which can be modified from toplevel **kwargs: θ (theta) controlling the mean reversion strength, μ (mu) defining the mean level, and σ (sigma) determining the noise scale. In this context, these parameters map to the temporal distance between sampled bins.

The implementation uses a discretized version of the OU stochastic differential equation, applied in a fashion better suited for a sampling procedure:

```drift = theta * (mean - current_position) * dt
drift = theta * (mean - current_position) * dt
randomness = scale * np.random.normal()
next_position = current_position + drift + randomness
```

Each generated path is then mapped to valid bin indices through normalization and clipping, ensuring the sampling remains will always yield a valid selection within the bin range. This provides a strategy, across epochs, that reliably fixates on a few arrays centrally within each epoch, which might overfit the dimensions stored within a set of a few time intervals of the signal. Across epochs, provided sensible per step deltas in KL-divergence are pre-defined before rerolling the process, this may or may not be more effective.

#### Rössler System Implementation

The Rössler system, a three-dimensional dynamical system exhibiting chaotic behavior, is adapted here for temporal movement by projecting its z-component onto our bin space. The system consists of the same three coupled derivatives as originally published by Otto Rössler in the 1970's.

The implementation here maps the z-component trajectory, again, scaled via interval wrapping to the number of total bins. The system's sensitivity to initial conditions is leveraged to generate varyingly periodic sampling patterns across the set of all bins, and the emergent chaotic attractor that this system provides ensures eventual convergence on stable patterns of motion (viewed from the standpoint of the derivative, anyway) for the broader the system in the long run.

#### Maximal Entropy Random Walk Implementation

The Maximal Entropy Random Walk (MERW) implementation constructs a graph where nodes represent bins and edges represent allowed transitions. Unlike simple random walks, MERW maximizes the entropy rate of the path selected while in motion, while maintaining uniform occupation of graph nodes in a stationary state. This is achieved in the script provided by using NumPy to construct an adjacency matrix representing the set of allowed transitions between nodes (in this case, corresponding to temporal bins), computing the principal eigenvector of said matrix, and then defining transition probabilities using the eigenvector components. 
This can be represented by the equation:

```
P[i,j] = (A[i,j] / λ) * (ψ[j] / ψ[i])
```

Where λ is the largest eigenvalue and ψ is the corresponding eigenvector. Note that any behavior emblematic of the unique properties of the MERW walker cannot occur on a k-regular graph, as uniformity within its constitutive lattice ensures that there is no optimal path to be taken - since the probability distribution governing the decision to choose any given path is unchanging in that scenario. Hence, I defined a "connectivity" parameter of 0.7, arbitrarily, which states the probability that an edge will be drawn connecting a given node to any other given node is 0.7.

The approach, which implements the ensures that the sampling process maximizes information gain per step while maintaining a more uniform coverage of the bin space than simple random sampling. The implementation uses sparse matrix operations for efficiency (for visualization, it uses Kamada-Kawai spacing for the nodes, which looked the best way to distribute the structure in two space, at least to my eye), and the resulting walk provides a way to ensure that the shuffling is governed via a stochastic search-like process, where our walker is motivated, despite its randomized starting position, to select, per step, maximally informative edges at a time interval when traveling across the set of all nodes.

### Model Design

At the time of writing the code for this toy implementation, the Liu et al (2024). preprint containing the brilliant Kolmogrov-Arnold Network design (which is the gold standard for future works of machine learning requiring interpretation, as it makes parameter-specific deltas directly accessible), so a more conventional structure, with some mathematically justifiable decisions and interpretable architectural choices, were implemented instead.

Broadly speaking, the model follows the traditional Variational Autoencoder (VAE) convention in its construction, a bunch of convolutional layers that decompose the inputs to a latent space of size 16 is used to construct the encoder, while the decoder extrapolates and deconvolves from the space. I applied this alongside a secondary choice I felt particular well-suited to my use-case. An Inverse Autoregressive Flow (IAF), using the API exemplar from TensorFlow Probability as its implementation. This provides the MADE neural network architecture to accelerate the requisite spline transformations to the posterior distributions of the hidden layers after each convolutional simplification (which is analyzed during the computation of KL-divergence) from a masked noise distribution. The rationale for this is largely as a means of coordinating the inference both with respect to history, the interdependent and nonrandom character of the distribution of timbral magnitude values across the frequency spectrum per unit time, and from a position of being epistemically agnostic to the form of learning the information itself (insofar as defining what constitutes the projected posterior distribution is concerned, anyway). After all, there's no guarantee that the way we integrate frequency information into the perceptual subclasses of "timbre" in any way always assumes the diagonality and stability of the prototypical unit gaussian. I'm sure there are certain frequency-dependent features in a given stream that our perceptual system is flexible and wise enough to attune itself to and fixate on. This exact selective fixedness, which appears present when looking, for instance, at child-directed attention to visual and auditory stimuli (see Wong et al, 2024), was explicitly what I was interested in accounting for. It's also meant to integrate within a larger theme of the work itself, which is actually the resuscitation of Kant's transcendental schematism as a viable object of empirical study that has the potential to open new doors within neuroinformatics and machine learning.

Prior to processing though, some reshaping of the nested data structure mentioned earlier was required for the model to be able to be compiled and ran, which can be seen through the prepare_data and prepare_tensorflow_dataset functions. The network is fed a two-dimensional structure at sample index t, with the first dimension corresponding to each bandpass filter, and the second corresponding to either the magnitude or phase values at that given frequency band. Each group of window_size bins constitutes a batch, and each bin itself is an epoch. The default run of 50 epochs was compared in the toy study (which, obviously, given the used window size of 256 at 44.1k [essentially required b/c of the VRAM limitations I was working within] corresponds to roughly 1/5 of a second of audio trained on altogether, hence the 5 runs of each stochastic method), but it's important to note that the code produced here was more of a supplemental exercise in "how" and "why" philosophically for the purposes of supplementing the thesis rather than a full exercise in training - which was not possible with the constraints given.

## Future Directions

As I didn't have the time within a course of a few months to properly test and benchmark the network with as much depth as I'd like to (due to the time costs associated with using an objectively bad platform like colab), there are lots of things I'd love to implement with this general protocol one day. While I used Seaborn to generate a heatmap of the parameter space at one of the hidden layers for visualization in the final thesis proof, there's an unfortunate lack of other metrics I was able to include. Precise analysis of multiple runs of the protocol in terms of KL-divergence deltas per sampling method, the steps required to inverse the fft transforms, and then to invert the filter space and mux the 100 bins of two inference runs to a single stereo output, was something I didn't have time to test or implement. And I won't, at least until I have a means of doing it away from the limitations of compute as a service.

Once a network like this has been trained sufficiently well to generalize to new information, and is well suited to reproduce the finer details of timbre at the limit of perception, I'd ideally like to run a study whereby self-reported preferences of subjects are A-B tested and evaluated linguistically according to the character of the reconstruction and its quality, so to verify the relative loadings of each latent_space dimension on actual codings of perception. I'd also add more interesting metrics - like spectrogram projections of the reconstructed output at various stages of the training process, alongside a more formal spectral analysis of window content. Ideally, we would see how pre-sorted spectral windows (timbrally) that evoke a specific linguistic connotation propagate through the network across epochs in terms of deltas in weight adjustment and prior adjustment. 